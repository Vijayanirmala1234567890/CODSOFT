! shred -u setup_google_colab.py
! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py
import setup_google_colab
# please, uncomment the week you're working on
# setup_google_colab.setup_week1()
# setup_google_colab.setup_week2()
# setup_google_colab.setup_week3()
# setup_google_colab.setup_week4()
# setup_google_colab.setup_week5()
setup_google_colab.setup_week6()
# If you're using the old version of the course (check a path of notebook on Coursera, you'll see v1 or v2),
# use setup_week2_old().

import sys
sys.path.append("..")
import grading
import download_utils

download_utils.link_all_keras_resources()

#Image Captioning
import tensorflow as tf
import keras
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt
L = keras.layers
K = keras.backend
import utils
import time
import zipfile
import json
from collections import defaultdict
import re
import random
from random import choice
import grading_utils
import os
from keras_utils import reset_tf_session
import tqdm_utils

# we downloaded them for you, just link them here
download_utils.link_week_6_resources()

IMG_SIZE = 299

# we take the last hidden layer of IncetionV3 as an image embedding
def get_cnn_encoder():
    K.set_learning_phase(False)
    model = keras.applications.InceptionV3(include_top=False)
    preprocess_for_model = keras.applications.inception_v3.preprocess_input
    model = keras.models.Model(model.inputs, keras.layers.GlobalAveragePooling2D()(model.output))
    return model, preprocess_for_model

# load prepared embeddings
train_img_embeds = utils.read_pickle("train_img_embeds.pickle")
train_img_fns = utils.read_pickle("train_img_fns.pickle")
val_img_embeds = utils.read_pickle("val_img_embeds.pickle")
val_img_fns = utils.read_pickle("val_img_fns.pickle")

# check shapes
print(train_img_embeds.shape, len(train_img_fns))
print(val_img_embeds.shape, len(val_img_fns))

# check prepared samples of images
list(filter(lambda x: x.endswith("_sample.zip"), os.listdir(".")))

# extract captions from zip
def get_captions_for_fns(fns, zip_fn, zip_json_path):
    zf = zipfile.ZipFile(zip_fn)
    j = json.loads(zf.read(zip_json_path).decode("utf8"))
    id_to_fn = {img["id"]: img["file_name"] for img in j["images"]}
    fn_to_caps = defaultdict(list)
    for cap in j['annotations']:
        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])
    fn_to_caps = dict(fn_to_caps)
    return list(map(lambda x: fn_to_caps[x], fns))
train_captions = get_captions_for_fns(train_img_fns, "captions_train-val2014.zip","annotations/captions_train2014.json")
# check shape
print(len(train_img_fns), len(train_captions))
print(len(val_img_fns), len(val_captions))

# look at training example (each has 5 captions)
def show_trainig_example(train_img_fns, train_captions, example_idx=0):
    zf = zipfile.ZipFile("train2014_sample.zip")
    captions_by_file = dict(zip(train_img_fns, train_captions))
    all_files = set(train_img_fns)
    found_files = list(filter(lambda x: x.filename.rsplit("/")[-1] in all_files, zf.filelist))
    example = found_files[example_idx]
    img = utils.decode_image_from_buf(zf.read(example))
    plt.imshow(utils.image_center_crop(img))
    plt.title("\n".join(captions_by_file[example.filename.rsplit("/")[-1]]))
    plt.show()
show_trainig_example(train_img_fns, train_captions, example_idx=142)

# preview captions data
train_captions[:2]

# special tokens
PAD = "#PAD#"
UNK = "#UNK#"
START = "#START#"
END = "#END#"
# split sentence into tokens (split into lowercased words)
def split_sentence(sentence):
    return list(filter(lambda x: len(x) > 0, re.split('\W+', sentence.lower())))
def generate_vocabulary(train_captions):
    vocab = {}
    counts = {}
    i = 0
    for captions in train_captions:
        for caption in captions:
            splitted = split_sentence(caption)
            for word in splitted:
                if word not in counts.keys():
                    counts[word] = 1
                else:
                    counts[word] += 1
                vocab[word] = i
                i += 1
    vocab = {k : v for k,v in vocab.items() if counts[k] >= 5}
    # We use padding for batching
    vocab[PAD] = i
    # Unknown words (not present in vocabulary)
    vocab[UNK] = i+1
    vocab[START] = i+2
    vocab[END] = i+3
    return {token: index for index, token in enumerate(sorted(vocab))}
def caption_tokens_to_indices(train_captions, vocab):
    res = []
    for i, captions in enumerate(train_captions):
        a = []
        for j, caption in enumerate(captions):
            splitted = split_sentence(caption)
            lis = []
            lis.append(vocab[START])
            for word in splitted:
                if word not in vocab.keys():
                    lis.append(vocab[UNK])
                else:
                    lis.append(vocab[word])
            lis.append(vocab[END])
            a.append(lis)
        res.append(a)
    return res

# prepare vocabulary
vocab = generate_vocabulary(train_captions)
vocab_inverse = {idx: w for w, idx in vocab.items()}
print(len(vocab))

# replace tokens with indices
train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)
val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)

# we will use this during training
def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):
    #batch_captions = np.array(batch_captions)
    lens = np.array([len(i) for i in batch_captions])
    if not max_len:
        max_len = max(lens)
    else:
        max_len = min(max_len, max(lens))
    matrix = [ caption[:max_len] + [pad_idx] * max(max_len-len(caption),0) for caption in batch_captions]
    return np.array(matrix)

# make sure you use correct argument in caption_tokens_to_indices
assert len(caption_tokens_to_indices(train_captions[:10], vocab)) == 10
assert len(caption_tokens_to_indices(train_captions[:5], vocab)) == 5

IMG_EMBED_SIZE = train_img_embeds.shape[1]
IMG_EMBED_BOTTLENECK = 120
WORD_EMBED_SIZE = 100
LSTM_UNITS = 300
LOGIT_BOTTLENECK = 120
pad_idx = vocab[PAD]

train_captions_indexed = np.array(train_captions_indexed)
val_captions_indexed = np.array(val_captions_indexed)

# generate batch via random sampling of images and captions for them,
# we use `max_len` parameter to control the length of the captions (truncating long captions)
def generate_batch(images_embeddings, indexed_captions, batch_size, max_len=None):
    indx_batch = np.random.choice(range(len(images_embeddings)), batch_size, replace=False)
    batch_image_embeddings = images_embeddings[indx_batch]### YOUR CODE HERE ###

    batch_captions = [captions[np.random.randint(5)] for captions in indexed_captions[indx_batch]]
    batch_captions_matrix =  batch_captions_to_matrix(batch_captions, pad_idx, max_len=max_len) ### YOUR CODE HERE ###

    return {decoder.img_embeds: batch_image_embeddings,
            decoder.sentences: batch_captions_matrix}

batch_size = 64
n_epochs = 2
n_batches_per_epoch = 1000
n_validation_batches = 100  # how many batches are used for validation after each epoch

!wget https://s3.ap-south-1.amazonaws.com/elasticbeanstalk-ap-south-1-081954957030/weights_7.data-00000-of-00001
!wget https://s3.ap-south-1.amazonaws.com/elasticbeanstalk-ap-south-1-081954957030/weights_7.index
!wget https://s3.ap-south-1.amazonaws.com/elasticbeanstalk-ap-south-1-081954957030/weights_7.meta

# look at how temperature works for probability distributions
# for high temperature we have more uniform distribution
_ = np.array([0.5, 0.4, 0.1])
for t in [0.01, 0.1, 1, 10, 100]:
    print(" ".join(map(str, _**(1/t) / np.sum(_**(1/t)))), "with temperature", t)

class final_model:
    # CNN encoder
    encoder, preprocess_for_model = get_cnn_encoder()
    # containers for current lstm state
    lstm_c = tf.Variable(tf.zeros([1, LSTM_UNITS]), name="cell")
    lstm_h = tf.Variable(tf.zeros([1, LSTM_UNITS]), name="hidden")

# this is an actual prediction loop
def generate_caption(image, t=1, sample=False, max_len=20):
    # condition lstm on the image
    s.run(final_model.init_lstm,
          {final_model.input_images: [image]})
    # current caption
    # start with only START token
    caption = [vocab[START]]
    for _ in range(max_len):
        next_word_probs = s.run(final_model.one_step,
                                {final_model.current_word: [caption[-1]]})[0]
        next_word_probs = next_word_probs.ravel()

        # apply temperature
        next_word_probs = next_word_probs**(1/t) / np.sum(next_word_probs**(1/t))

        if sample:
            next_word = np.random.choice(range(len(vocab)), p=next_word_probs)
        else:
            next_word = np.argmax(next_word_probs)

        caption.append(next_word)
        if next_word == vocab[END]:
            break

    return list(map(vocab_inverse.get, caption)

# look at validation prediction example
def apply_model_to_image_raw_bytes(raw):
    img = utils.decode_image_from_buf(raw)
    fig = plt.figure(figsize=(7, 7))
    plt.grid('off')
    plt.axis('off')
    plt.imshow(img)
    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)
    plt.show()

def show_valid_example(val_img_fns, example_idx=0):
    zf = zipfile.ZipFile("val2014_sample.zip")
    all_files = set(val_img_fns)
    found_files = list(filter(lambda x: x.filename.rsplit("/")[-1] in all_files, zf.filelist))
    example = found_files[example_idx]
    apply_model_to_image_raw_bytes(zf.read(example))

show_valid_example(val_img_fns, example_idx=100)

download_utils.download_file(
    "http://www.bijouxandbits.com/wp-content/uploads/2016/06/portal-cake-10.jpg",
    "portal-cake-10.jpg"
)

apply_model_to_image_raw_bytes(open("portal-cake-10.jpg", "rb").read())
